import os
import streamlit as st
import streamlit.components.v1 as components
import openai
import base64
import backoff
import tiktoken
import time
import pandas as pd
import matplotlib.pyplot as plt
import feedparser
import requests
from datetime import datetime, date
from urllib.parse import urlencode, quote_plus
from dotenv import load_dotenv
from llama_index.core import (
    VectorStoreIndex, SimpleDirectoryReader,
    StorageContext, load_index_from_storage, Settings
)
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.llms.openai import OpenAI
import logging, traceback
from io import BytesIO
from PIL import Image

load_dotenv()

# â”€â”€â”€ API í‚¤ë“¤ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
openai.api_key = (
    st.secrets.get("OPENAI_API_KEY")
    or os.getenv("OPENAI_API_KEY", "")
)
API_KEY      = os.getenv("ODCLOUD_API_KEY")
HF_API_TOKEN = os.getenv("HF_API_TOKEN")
HF_API_URL   = os.getenv("HF_API_URL")

# â”€â”€â”€ 1) ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ (Google News RSS) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data(ttl=300)
def fetch_google_news(keyword: str, max_items: int = 10):
    clean_kw = " ".join(keyword.strip().split())
    params = {"q": clean_kw, "hl": "ko", "gl": "KR", "ceid": "KR:ko"}
    rss_url = "https://news.google.com/rss/search?" + urlencode(params, doseq=True)
    feed = feedparser.parse(rss_url)
    items = []
    for entry in feed.entries[:max_items]:
        pub_date = datetime(*entry.published_parsed[:6]).strftime("%Y-%m-%d")
        source   = entry.get("source", {}).get("title", "")
        items.append({
            "title":  entry.title,
            "link":   entry.link,
            "source": source,
            "date":   pub_date,
        })
    return items

# â”€â”€â”€ 2) ì„ ë°• ê´€ì œì •ë³´ ì¡°íšŒ ì„¹ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def vessel_monitoring_section():
    st.subheader("ğŸš¢ í•´ì–‘ìˆ˜ì‚°ë¶€ ì„ ë°• ê´€ì œì •ë³´ ì¡°íšŒ")
    date_from = st.date_input("ì¡°íšŒ ì‹œì‘ì¼", date.today())
    date_to   = st.date_input("ì¡°íšŒ ì¢…ë£Œì¼", date.today())
    page      = st.number_input("í˜ì´ì§€ ë²ˆí˜¸", 1, 1000, 1)
    per_page  = st.slider("í•œ ë²ˆì— ê°€ì ¸ì˜¬ ê±´ìˆ˜", 1, 1000, 100)
    if st.button("ğŸ” ì¡°íšŒ"):
        params = {
            "serviceKey": API_KEY,
            "page":       page,
            "perPage":    per_page,
            "fromDate":   date_from.strftime("%Y-%m-%d"),
            "toDate":     date_to.strftime("%Y-%m-%d"),
        }
        with st.spinner("ì¡°íšŒ ì¤‘â€¦"):
            res = requests.get(
                "https://api.odcloud.kr/api/15128156/v1/uddi:fdcdb0d1-0296-4c3b-8087-8ab4bd4d5123",
                params=params
            )
        if res.status_code != 200:
            st.error(f"API ì˜¤ë¥˜ {res.status_code}")
            return
        data = res.json().get("data", [])
        if data:
            df = pd.DataFrame(data)
            st.success(f"ì´ {len(df)} ê±´ ì¡°íšŒë˜ì—ˆìŠµë‹ˆë‹¤.")
            st.dataframe(df)
        else:
            st.warning("ì¡°íšŒëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

# â”€â”€â”€ 3) ì˜¤ëŠ˜ì˜ ë‚ ì”¨ ì„¹ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def today_weather_section():
    st.subheader("â˜€ï¸ ì˜¤ëŠ˜ì˜ ë‚ ì”¨ ì¡°íšŒ")
    city_name = st.text_input("ë„ì‹œ ì´ë¦„ ì…ë ¥ (ì˜ˆ: ì„œìš¸, Busan)")
    if st.button("ğŸ” ë‚ ì”¨ ê°€ì ¸ì˜¤ê¸°"):
        if not city_name:
            st.warning("ë„ì‹œ ì´ë¦„ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
            return

        q_name  = quote_plus(city_name)
        geo_url = f"https://geocoding-api.open-meteo.com/v1/search?name={q_name}&count=5&language=ko"
        with st.spinner("ìœ„ì¹˜ ê²€ìƒ‰ ì¤‘â€¦"):
            geo_res = requests.get(geo_url)
        if geo_res.status_code != 200:
            st.error("ì§€ì˜¤ì½”ë”© API ì˜¤ë¥˜")
            return
        results = geo_res.json().get("results")
        if not results:
            st.warning("ë„ì‹œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        loc          = results[0]
        lat, lon     = loc["latitude"], loc["longitude"]
        display_name = f"{loc['name']}, {loc['country']}"

        weather_url = (
            f"https://api.open-meteo.com/v1/forecast?"
            f"latitude={lat}&longitude={lon}&current_weather=true"
            f"&hourly=relativehumidity_2m&timezone=auto"
        )
        with st.spinner(f"{display_name} ë‚ ì”¨ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘â€¦"):
            w_res = requests.get(weather_url)
        if w_res.status_code != 200:
            st.error("ë‚ ì”¨ API ì˜¤ë¥˜")
            return

        js   = w_res.json()
        cw   = js.get("current_weather", {})
        temp, wind_spd, wind_dir, code = (
            cw.get("temperature"),
            cw.get("windspeed"),
            cw.get("winddirection"),
            cw.get("weathercode"),
        )
        wc_map = {
            0:"ë§‘ìŒ",1:"ì£¼ë¡œ ë§‘ìŒ",2:"ë¶€ë¶„ì  êµ¬ë¦„",3:"êµ¬ë¦„ ë§ìŒ",
            45:"ì•ˆê°œ",48:"ì•ˆê°œ(ì…ìƒ)",
            51:"ì´ìŠ¬ë¹„ ì•½í•¨",53:"ì´ìŠ¬ë¹„ ë³´í†µ",55:"ì´ìŠ¬ë¹„ ê°•í•¨",
            61:"ë¹—ë°©ìš¸ ì•½í•¨",63:"ë¹—ë°©ìš¸ ë³´í†µ",65:"ë¹—ë°©ìš¸ ê°•í•¨",
            80:"ì†Œë‚˜ê¸° ì•½í•¨",81:"ì†Œë‚˜ê¸° ë³´í†µ",82:"ì†Œë‚˜ê¸° ê°•í•¨",
            95:"ë‡Œìš°",96:"ì•½í•œ ë‡Œìš°",99:"ê°•í•œ ë‡Œìš°"
        }
        desc      = wc_map.get(code, "ì•Œ ìˆ˜ ì—†ìŒ")
        times     = js["hourly"]["time"]
        hums      = js["hourly"]["relativehumidity_2m"]
        now       = datetime.now().strftime("%Y-%m-%dT%H:00")
        humidity  = hums[times.index(now)] if now in times else None

        st.markdown(f"### {display_name} í˜„ì¬ ë‚ ì”¨")
        c1, c2, c3, c4 = st.columns(4)
        c1.metric("ğŸŒ¡ï¸ ê¸°ì˜¨(â„ƒ)", temp)
        c2.metric("ğŸ’¨ í’ì†(m/s)", wind_spd)
        c3.metric("ğŸŒ«ï¸ í’í–¥(Â°)", wind_dir)
        c4.metric("ğŸ’§ ìŠµë„(%)", humidity or "â€“")
        st.markdown(f"**ë‚ ì”¨ ìƒíƒœ:** {desc}")

# â”€â”€â”€ 4) Chatbot (Vision) & ìš”ì•½ ê¸°ëŠ¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
enc = tiktoken.encoding_for_model("gpt-4o")
MAX_TOKENS = 262144     # gpt-4o ëŒ€ëµ 262K í† í° í—ˆìš©
SUMMARY_THRESHOLD = 40  # ëŒ€í™” ë©”ì‹œì§€(turn)ê°€ 40ê°œ ì´ìƒ ë„˜ì–´ê°€ë©´ ìš”ì•½

def num_tokens(messages: list) -> int:
    total = 0
    for m in messages:
        if isinstance(m["content"], list):
            for blk in m["content"]:
                if blk["type"] == "text":
                    total += len(enc.encode(blk["text"]))
                elif blk["type"] == "image_url":
                    total += len(enc.encode(blk["image_url"]["url"]))
        else:
            total += len(enc.encode(m["content"]))
    return total

@backoff.on_exception(backoff.expo, openai.RateLimitError, max_time=60, jitter=None)
def safe_chat_completion(messages, model="gpt-4o"):
    tk_in = num_tokens(messages)
    if tk_in > MAX_TOKENS:
        raise ValueError(f"ì…ë ¥ í† í° {tk_in}ê°œ â†’ ìµœëŒ€ í—ˆìš©ì¹˜({MAX_TOKENS}) ì´ˆê³¼ì…ë‹ˆë‹¤.")
    return openai.chat.completions.create(
        model=model,
        messages=messages,
        max_tokens=300,
        stream=True
    )

def compress_image(file, max_px=768, quality=85):
    img = Image.open(file)
    if max(img.size) > max_px:
        ratio = max_px / max(img.size)
        img = img.resize((int(img.width * ratio), int(img.height * ratio)), Image.LANCZOS)
    buf = BytesIO()
    img.convert("RGB").save(buf, format="JPEG", quality=quality)
    return buf.getvalue()

def summarize_history(history: list) -> str:
    """
    ì „ì²´ history(messages)ë¥¼ ì§§ê²Œ ìš”ì•½í•˜ì—¬ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜.
    GPTì—ê²Œ ìš”ì•½ ìš”ì²­ì„ ë³´ë‚´ê³  3ë¬¸ì¥ ì´ë‚´ ìš”ì•½ë¬¸ì„ ë°›ì•„ì˜´.
    """
    prompt = [{"role": "system", "content": "ì•„ë˜ ëŒ€í™”ë¥¼ ì§§ê²Œ ìš”ì•½í•´ ì£¼ì„¸ìš”."}]
    prompt += history + [{"role": "user", "content": "ì, ì´ ëŒ€í™” ë‚´ìš©ì„ 3ë¬¸ì¥ ì´ë‚´ë¡œ ìš”ì•½í•´ ì¤˜."}]
    res = openai.chat.completions.create(
        model="gpt-4o-mini",
        messages=prompt,
        max_tokens=200
    )
    return res.choices[0].message.content  # ìš”ì•½ëœ í…ìŠ¤íŠ¸

def chatgpt_clone_section():
    st.subheader("ğŸ’¬ Chatbot (Vision)")
    img_file = st.file_uploader("ğŸ–¼ï¸ ì´ë¯¸ì§€ (ì„ íƒ)", type=["png", "jpg", "jpeg"])
    prompt   = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”")

    # ì„¸ì…˜ ìŠ¤í…Œì´íŠ¸ ì´ˆê¸°í™”
    st.session_state.setdefault("chat_history", [])

    # 1) ëŒ€í™” ê¸°ë¡ì´ SUMMARY_THRESHOLD í„´ì„ ì´ˆê³¼í•˜ë©´ ìš”ì•½ ìˆ˜í–‰
    if len(st.session_state.chat_history) > SUMMARY_THRESHOLD:
        try:
            summary = summarize_history(st.session_state.chat_history)
            # ìš”ì•½ëœ í…ìŠ¤íŠ¸ë¥¼ assistant ì—­í• ë¡œ ì €ì¥ í›„, historyë¥¼ ì¬êµ¬ì„±
            st.session_state.chat_history = [
                {"role": "assistant", "content": summary}
            ]
        except Exception as e:
            st.error(f"ëŒ€í™” ìš”ì•½ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            return

    # 2) í™”ë©´ì— ì´ì „ ëŒ€í™” ë‚´ìš© í‘œì‹œ
    for msg in st.session_state.chat_history:
        role = msg["role"]
        content = msg["content"]
        if role == "user":
            with st.chat_message("user"):
                # contentê°€ ë¸”ë¡ ë¦¬ìŠ¤íŠ¸ì¼ ê²½ìš° ì²˜ë¦¬
                if isinstance(content, list):
                    for blk in content:
                        if blk["type"] == "text":
                            st.write(blk["text"])
                        elif blk["type"] == "image_url":
                            st.image(blk["image_url"]["url"], caption="ì‚¬ìš©ì ì—…ë¡œë“œ ì´ë¯¸ì§€")
                else:
                    st.write(content)
        else:  # assistant
            with st.chat_message("assistant"):
                st.write(content)

    # 3) ì…ë ¥ì´ ì—†ìœ¼ë©´ ë¦¬í„´
    if img_file is None and not prompt:
        return

    # 4) ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
    user_blocks = []
    if prompt:
        user_blocks.append({"type": "text", "text": prompt})
    if img_file:
        jpg_bytes = compress_image(img_file)
        st.image(jpg_bytes, caption=f"ë¯¸ë¦¬ë³´ê¸° ({len(jpg_bytes)//1024} KB)", use_container_width=True)
        b64 = base64.b64encode(jpg_bytes).decode()
        img_block = {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{b64}"}}
        user_blocks.append(img_block)

    # 5) ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ chat_historyì— ì¶”ê°€
    st.session_state.chat_history.append({"role": "user", "content": user_blocks})

    # 6) ëª¨ë¸ì—ê²Œ ë³´ë‚¼ ë©”ì‹œì§€(prospective)ë¥¼ êµ¬ì„± í›„ í† í° ê²€ì¦
    prospective = st.session_state.chat_history.copy()
    tk_in = num_tokens(prospective)
    if tk_in > MAX_TOKENS:
        st.error(f"í˜„ì¬ ëŒ€í™” í† í° ìˆ˜({tk_in})ê°€ ë„ˆë¬´ ë§ì•„ í˜¸ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
                 "ì˜¤ë˜ëœ ëŒ€í™”ë¥¼ ìš”ì•½í•˜ê±°ë‚˜ ì¼ë¶€ ë©”ì‹œì§€ë¥¼ ì œê±°í•´ ì£¼ì„¸ìš”.")
        return

    # 7) GPT í˜¸ì¶œ ë° ì¶œë ¥
    try:
        resp = safe_chat_completion(st.session_state.chat_history)
        buf = ""
        # assistant ë©”ì‹œì§€ë¥¼ ë¯¸ë¦¬ ì¶”ê°€í•´ë‘ê³ , ë‚´ìš©ì„ ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ê³„ì† ë§ë¶™ì„
        st.session_state.chat_history.append({"role": "assistant", "content": ""})
        with st.chat_message("assistant"):
            ph = st.empty()
            for chunk in resp:
                delta = chunk.choices[0].delta.content
                if delta:
                    buf += delta
                    ph.markdown(buf + "â–Œ")
            ph.markdown(buf)
        # ì™„ì„±ëœ assistant ì‘ë‹µì„ ì„¸ì…˜ì— ë°˜ì˜
        st.session_state.chat_history[-1]["content"] = buf
    except openai.RateLimitError:
        st.error("â³ ë ˆì´íŠ¸ ë¦¬ë°‹ì— ê±¸ë ¸ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")
    except Exception as e:
        st.error(f"OpenAI í˜¸ì¶œ ì˜¤ë¥˜: {e}")

# â”€â”€â”€ 5) ëŒ“ê¸€ ì„¹ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def comments_section():
    """
    ë¡œì»¬ CSV íŒŒì¼(comments.csv)ì„ ì‚¬ìš©í•˜ì—¬ ëŒ“ê¸€ì„ ì €ì¥í•˜ê³ , ë³´ì—¬ì£¼ëŠ” ì„¹ì…˜.
    """
    st.subheader("ğŸ—¨ï¸ ëŒ“ê¸€ ë‚¨ê¸°ê¸°")

    # 1) ëŒ“ê¸€ íŒŒì¼ ê²½ë¡œ ì„¤ì •
    comments_file = "comments.csv"

    # 2) ëŒ“ê¸€ì„ ì €ì¥í•  CSV íŒŒì¼ì´ ì—†ìœ¼ë©´ í—¤ë”ë§Œ ìƒì„±
    if not os.path.exists(comments_file):
        df_init = pd.DataFrame(columns=["timestamp", "name", "comment"])
        df_init.to_csv(comments_file, index=False, encoding="utf-8-sig")

    # 3) ëŒ“ê¸€ì„ ì…ë ¥ë°›ì„ UI (ì´ë¦„, ëŒ“ê¸€ ë‚´ìš©, ë“±ë¡ ë²„íŠ¼)
    with st.form(key="comment_form", clear_on_submit=True):
        name = st.text_input("ì´ë¦„", max_chars=50)
        comment = st.text_area("ëŒ“ê¸€ ë‚´ìš©", height=100, max_chars=500)
        submitted = st.form_submit_button("ë“±ë¡")

    # 4) ì‚¬ìš©ìê°€ ì œì¶œ ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ CSVì— ì €ì¥
    if submitted:
        if not name.strip():
            st.warning("ì´ë¦„ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
        elif not comment.strip():
            st.warning("ëŒ“ê¸€ ë‚´ìš©ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
        else:
            # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
            ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            # ìƒˆë¡œìš´ ëŒ“ê¸€ DataFrame
            new_row = pd.DataFrame([{
                "timestamp": ts,
                "name": name.strip(),
                "comment": comment.strip()
            }])
            # CSVì— ì´ì–´ë¶™ì´ê¸°
            new_row.to_csv(comments_file, mode="a", header=False, index=False, encoding="utf-8-sig")
            st.success("ëŒ“ê¸€ì´ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤!")

    # 5) ì €ì¥ëœ ëª¨ë“  ëŒ“ê¸€ì„ ì½ì–´ì„œ í™”ë©´ì— í‘œì‹œ
    try:
        all_comments = pd.read_csv(comments_file, encoding="utf-8-sig")
        # ìµœì‹ ìˆœìœ¼ë¡œ í‘œì‹œí•˜ë ¤ë©´ ì•„ë˜ì²˜ëŸ¼ ì •ë ¬
        all_comments = all_comments.sort_values(by="timestamp", ascending=False)
        st.markdown("#### ì „ì²´ ëŒ“ê¸€")
        for _, row in all_comments.iterrows():
            st.markdown(f"- **[{row['timestamp']}] {row['name']}**: {row['comment']}")
    except Exception as e:
        st.error(f"ëŒ“ê¸€ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

# â”€â”€â”€ 6) â€œESG í™œë™ ì°¸ì—¬â€ ì„¹ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_table_download_link(df: pd.DataFrame, filename: str = "participation.csv"):
    """
    pandas DataFrameì„ CSVë¡œ ë³€í™˜ í›„, Streamlit ë‹¤ìš´ë¡œë“œ ë§í¬ HTML ìƒì„±
    """
    csv = df.to_csv(index=False, encoding="utf-8-sig")
    b64 = base64.b64encode(csv.encode()).decode()  # ë°”ì´ë„ˆë¦¬ ë°ì´í„°ë¥¼ base64ë¡œ ì¸ì½”ë”©
    href = f'<a href="data:file/csv;base64,{b64}" download="{filename}">ğŸ“¥ CSV ë‹¤ìš´ë¡œë“œ</a>'
    return href

def participation_section():
    st.subheader("ğŸ–Šï¸ ESG í™œë™ ì°¸ì—¬")
    img_dir = "participation_images"
    csv_file = "participation.csv"

    # 1) ë””ë ‰í„°ë¦¬ ë° CSV íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒì„±
    if not os.path.exists(img_dir):
        os.makedirs(img_dir)
    if not os.path.exists(csv_file):
        df_init = pd.DataFrame(columns=["timestamp", "department", "name", "image_filename"])
        df_init.to_csv(csv_file, index=False, encoding="utf-8-sig")

    # 2) Streamlit form ìƒì„± (ë¶€ì„œ, ì„±ëª…, ì´ë¯¸ì§€)
    with st.form(key="participation_form", clear_on_submit=True):
        dept = st.text_input("ì°¸ì—¬ ë¶€ì„œ", max_chars=50, help="ì˜ˆ: ë¬¼ë¥˜íŒ€, ì˜ì—…ë¶€ ë“±")
        person = st.text_input("ì„±ëª…", max_chars=30)
        uploaded_file = st.file_uploader("ì¦ëª…ìë£Œ(ì´ë¯¸ì§€)", type=["png", "jpg", "jpeg"])
        submit_button = st.form_submit_button("ì œì¶œ")

    # 3) ì œì¶œ ë²„íŠ¼ì´ ëˆŒë¦¬ë©´ ë¡œì»¬ì— ì €ì¥ í›„ CSVì— ê¸°ë¡
    if submit_button:
        if not dept.strip():
            st.warning("ì°¸ì—¬ ë¶€ì„œë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
        elif not person.strip():
            st.warning("ì„±ëª…ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
        elif uploaded_file is None:
            st.warning("ì´ë¯¸ì§€ íŒŒì¼ì„ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”.")
        else:
            # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
            ts = datetime.now().strftime("%Y%m%d_%H%M%S")
            ext = os.path.splitext(uploaded_file.name)[1].lower()  # ì˜ˆ: ".jpg"
            safe_person = "".join(person.split())  # ê³µë°± ì œê±°
            img_filename = f"{ts}_{safe_person}{ext}"
            img_path = os.path.join(img_dir, img_filename)

            # ì´ë¯¸ì§€ ë¡œì»¬ì— ì €ì¥
            with open(img_path, "wb") as f:
                f.write(uploaded_file.getbuffer())

            # CSVì— ìƒˆë¡œìš´ í–‰ ì¶”ê°€
            new_row = pd.DataFrame([{
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "department": dept.strip(),
                "name": person.strip(),
                "image_filename": img_filename
            }])
            new_row.to_csv(csv_file, mode="a", header=False, index=False, encoding="utf-8-sig")

            st.success("âœ… ì°¸ì—¬ ì •ë³´ê°€ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤!")

    # 4) ì €ì¥ëœ CSV ë¶ˆëŸ¬ì™€ DataFrameìœ¼ë¡œ ì½ê¸°
    try:
        all_data = pd.read_csv(csv_file, encoding="utf-8-sig").sort_values(
            by="timestamp", ascending=False
        )

        # 4-1) CSV ë‹¤ìš´ë¡œë“œ ë§í¬ í‘œì‹œ
        st.markdown(
            get_table_download_link(all_data, filename="participation.csv"),
            unsafe_allow_html=True
        )

        # 4-2) í™”ë©´ì— í‘œë¡œ ì¶œë ¥
        st.dataframe(all_data)

        # 4-3) ì´ë¯¸ì§€ ì¸ë„¤ì¼ + ë¶€ì„œ/ì„±ëª… ì¶œë ¥
        for _, row in all_data.iterrows():
            col1, col2 = st.columns([1, 3])
            with col1:
                img_path = os.path.join(img_dir, row["image_filename"])
                if os.path.exists(img_path):
                    st.image(img_path, width=80)
                else:
                    st.write("(ì´ë¯¸ì§€ ì—†ìŒ)")
            with col2:
                st.write(f"- **[{row['timestamp']}]** {row['department']} / {row['name']}")
    except Exception as e:
        st.error(f"ì°¸ì—¬ í˜„í™©ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

# â”€â”€â”€ 7) ì˜ìƒ ëª¨ìŒ ì„¹ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def video_collection_section():
    st.subheader("ğŸ“º ESG ì˜ìƒ ëª¨ìŒ")
    # 1. ì‚¬ë¬´ì‹¤ì—ì„œ ì´ë©´ì§€ í™œìš©í•˜ê¸°!
    st.markdown("#### ì‚¬ë¬´ì‹¤ì—ì„œ ì´ë©´ì§€ í™œìš©í•˜ê¸°!")
    st.video("https://storage.googleapis.com/videoupload_icpa/%EC%82%AC%EB%AC%B4%EC%8B%A4%EC%97%90%EC%84%9C%20%EC%9D%B4%EB%A9%B4%EC%A7%80%20%ED%99%9C%EC%9A%A9%ED%95%98%EA%B8%B0.mp4")
    st.write("")  # ì¤„ ê°„ê²©

    # 2. ì¹´í˜ì—ì„œ ESG ì‹¤ì²œí•˜ê¸° 1íƒ„
    st.markdown("#### ì¹´í˜ì—ì„œ ESG ì‹¤ì²œí•˜ê¸° 1íƒ„")
    st.video("https://storage.googleapis.com/videoupload_icpa/%EC%B9%B4%ED%8E%98%EC%97%90%EC%84%9C%20%ED%85%80%EB%B8%94%EB%9F%AC%EB%8A%94%20%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0.mp4")
    st.write("")

    # 3. ì¹´í˜ì—ì„œ íœ´ì§€ ì ê²Œ ì‚¬ìš©í•˜ê¸°
    st.markdown("#### ì¹´í˜ì—ì„œ íœ´ì§€ ì ê²Œ ì‚¬ìš©í•˜ê¸°")
    st.video("https://storage.googleapis.com/videoupload_icpa/%EC%B9%B4%ED%8E%98%EC%97%90%EC%84%9C%20%ED%9C%B4%EC%A7%80%20%EC%A0%81%EA%B2%8C%20%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0.mp4")

# â”€â”€â”€ 8) ì•± ë ˆì´ì•„ì›ƒ (íƒ­ êµ¬ì„±) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="ì¸ì²œí•­ë§Œê³µì‚¬ ESG í†µí•© í¬í„¸", layout="centered")
st.title("ğŸ“ˆ ì¸ì²œí•­ë§Œê³µì‚¬ ESG í†µí•© í¬í„¸: ë‰´ìŠ¤Â·ì„ ë°•Â·ë‚ ì”¨Â·ChatbotÂ·ëŒ“ê¸€Â·ESG í™œë™ ì°¸ì—¬Â·ESG ì˜ìƒ ëª¨ìŒ")

tabs = st.tabs([
    "êµ¬ê¸€ ë‰´ìŠ¤", "ì„ ë°• ê´€ì œì •ë³´", "ì˜¤ëŠ˜ì˜ ë‚ ì”¨", "Chatbot", "ëŒ“ê¸€", "ESG í™œë™ ì°¸ì—¬", "ESG ì˜ìƒ ëª¨ìŒ"
])

with tabs[0]:
    st.subheader("â–¶ êµ¬ê¸€ ë‰´ìŠ¤ í¬ë¡¤ë§ (RSS)")
    kw  = st.text_input("ê²€ìƒ‰ í‚¤ì›Œë“œ", "ESG")
    num = st.slider("ê°€ì ¸ì˜¬ ê¸°ì‚¬ ê°œìˆ˜", 5, 50, 10)
    if st.button("ë³´ê¸°", key="news_btn"):
        for it in fetch_google_news(kw, num):
            st.markdown(f"- **[{it['source']} Â· {it['date']}]** [{it['title']}]({it['link']})")

with tabs[1]:
    vessel_monitoring_section()

with tabs[2]:
    today_weather_section()

with tabs[3]:
    chatgpt_clone_section()

with tabs[4]:
    comments_section()

with tabs[5]:
    participation_section()

with tabs[6]:
    video_collection_section()


















