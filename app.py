# app.py  |  Streamlit í†µí•© ë°ëª¨
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import feedparser
from bs4 import BeautifulSoup
from datetime import datetime
from urllib.parse import quote_plus  # URL ì¸ì½”ë”©ìš©

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1) ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ (Google News RSS)
@st.cache_data(ttl=300)
def fetch_google_news(keyword: str, max_items: int = 10):
    """
    Google News RSS í”¼ë“œì—ì„œ keyword ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìµœëŒ€ max_itemsê°œ ê°€ì ¸ì˜¨ë‹¤.
    ë°˜í™˜: [{title, link, source, date}, â€¦]
    """
    # ë„ì–´ì“°ê¸°ê°€ ìˆëŠ” ê²€ìƒ‰ì–´ë„ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
    encoded = quote_plus(keyword)
    rss_url = (
        "https://news.google.com/rss/search?"
        f"q={encoded}&hl=ko&gl=KR&ceid=KR:ko"
    )
    feed = feedparser.parse(rss_url)

    items = []
    for entry in feed.entries[:max_items]:
        pub_date = datetime(*entry.published_parsed[:6]).strftime("%Y-%m-%d")
        source = entry.source.title if "source" in entry else ""
        items.append({
            "title": entry.title,
            "link" : entry.link,
            "source": source,
            "date"  : pub_date,
        })
    return items

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2) CSV íˆìŠ¤í† ê·¸ë¨ ì„¹ì…˜
def sample_data_section():
    st.subheader("ğŸ“Š ìƒ˜í”Œ ë°ì´í„° íˆìŠ¤í† ê·¸ë¨")
    uploaded_file = st.file_uploader("CSV íŒŒì¼ ì—…ë¡œë“œ (optional)", type=["csv"])
    if uploaded_file:
        df = pd.read_csv(uploaded_file)
        st.write(df)

        numeric_cols = df.select_dtypes(include="number").columns.tolist()
        if numeric_cols:
            col = st.selectbox("Numeric ì»¬ëŸ¼ ì„ íƒ", numeric_cols)
            fig, ax = plt.subplots()
            ax.hist(df[col], bins=10)
            ax.set_xlabel(col)
            ax.set_ylabel("Frequency")
            st.pyplot(fig)
    else:
        st.info("CSV íŒŒì¼ì„ ì˜¬ë¦¬ë©´ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°ì™€ íˆìŠ¤í† ê·¸ë¨ì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3) ë™ì˜ìƒ ì—…ë¡œë“œÂ·ì¬ìƒ ì„¹ì…˜
def video_upload_section():
    st.subheader("ğŸ“¹ ë™ì˜ìƒ ì—…ë¡œë“œ & ì¬ìƒ")
    video_file = st.file_uploader(
        "ë™ì˜ìƒ íŒŒì¼ ì—…ë¡œë“œ (MP4 / MOV / AVI)",
        type=["mp4", "mov", "avi"],
        key="video_uploader",
    )
    if video_file:
        st.video(video_file)
    else:
        st.info("ìœ„ ë²„íŠ¼ìœ¼ë¡œ ë™ì˜ìƒ íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”.")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4) ì•± ë ˆì´ì•„ì›ƒ (íƒ­ êµ¬ì„±)
st.set_page_config(page_title="í†µí•© ë°ëª¨", layout="centered")
st.title("ğŸ“ˆ í†µí•© ë°ëª¨: êµ¬ê¸€ ë‰´ìŠ¤ Â· ë°ì´í„° Â· ë™ì˜ìƒ")

tab_news, tab_hist, tab_vid = st.tabs(
    ["êµ¬ê¸€ ë‰´ìŠ¤", "ë°ì´í„° íˆìŠ¤í† ê·¸ë¨", "ë™ì˜ìƒ ì¬ìƒ"]
)

with tab_news:
    st.subheader("â–¶ êµ¬ê¸€ ë‰´ìŠ¤ í¬ë¡¤ë§ (RSS)")
    keyword = st.text_input("ê²€ìƒ‰ í‚¤ì›Œë“œ", value="ESG", key="kw_input")
    num     = st.slider("ê°€ì ¸ì˜¬ ê¸°ì‚¬ ê°œìˆ˜", 5, 20, 10, key="num_slider")

    if st.button("ìµœì‹  ë‰´ìŠ¤ ë³´ê¸°", key="news_btn"):
        with st.spinner(f"â€˜{keyword}â€™ ë‰´ìŠ¤ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘â€¦"):
            news_items = fetch_google_news(keyword, num)

        if news_items:
            for item in news_items:
                st.markdown(
                    f"- **[{item['source']} Â· {item['date']}]** "
                    f"[{item['title']}]({item['link']})"
                )
        else:
            st.warning("ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

with tab_hist:
    sample_data_section()

with tab_vid:
    video_upload_section()

