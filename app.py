import os
import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import feedparser
import requests
from datetime import datetime, date
from urllib.parse import urlencode
from urllib.parse import urlencode, quote_plus  # â† quote_plus ì¶”ê°€

API_KEY = os.getenv("ODCLOUD_API_KEY", "GprdI3W07y8Ul7R0KwyRE0Beb1Y2wqtlBuvzWRqLqIZzEkR7xrPePc6CMQeD9FQAsTyQHh1V8NDK1md4ou4WGw==")

# Hugging Face Inference API ì„¤ì •
HF_API_TOKEN = os.getenv("HF_API_TOKEN", "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")
HF_API_URL   = os.getenv("HF_API_URL", "https:////huggingface.co/kakaocorp/kanana-nano-2.1b-instruct")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1) ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ (Google News RSS) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data(ttl=300)
def fetch_google_news(keyword: str, max_items: int = 10):
    clean_kw = " ".join(keyword.strip().split())
    params = {"q": clean_kw, "hl": "ko", "gl": "KR", "ceid": "KR:ko"}
    rss_url = "https://news.google.com/rss/search?" + urlencode(params, doseq=True)
    feed = feedparser.parse(rss_url)
    items = []
    for entry in feed.entries[:max_items]:
        pub_date = datetime(*entry.published_parsed[:6]).strftime("%Y-%m-%d")
        source   = entry.get("source", {}).get("title", "")
        items.append({
            "title": entry.title,
            "link":  entry.link,
            "source": source,
            "date":   pub_date,
        })
    return items

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2) CSV íˆìŠ¤í† ê·¸ë¨ ì„¹ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def sample_data_section():
    st.subheader("ğŸ“Š ìƒ˜í”Œ ë°ì´í„° íˆìŠ¤í† ê·¸ë¨")
    uploaded_file = st.file_uploader("CSV íŒŒì¼ ì—…ë¡œë“œ (optional)", type=["csv"], key="hist_csv")
    if uploaded_file:
        df = pd.read_csv(uploaded_file)
        st.dataframe(df)
        nums = df.select_dtypes(include="number").columns.tolist()
        if nums:
            col = st.selectbox("Numeric ì»¬ëŸ¼ ì„ íƒ", nums, key="hist_col")
            fig, ax = plt.subplots()
            ax.hist(df[col], bins=10)
            ax.set_xlabel(col)
            ax.set_ylabel("Frequency")
            st.pyplot(fig)
    else:
        st.info("CSV íŒŒì¼ì„ ì˜¬ë¦¬ë©´ íˆìŠ¤í† ê·¸ë¨ì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3) ë™ì˜ìƒ ì—…ë¡œë“œÂ·ì¬ìƒ ì„¹ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def video_upload_section():
    st.subheader("ğŸ“¹ ë™ì˜ìƒ ì—…ë¡œë“œ & ì¬ìƒ")
    video_file = st.file_uploader("ë™ì˜ìƒ íŒŒì¼ ì—…ë¡œë“œ (MP4/MOV/AVI)", type=["mp4","mov","avi"], key="vid_file")
    if video_file:
        st.video(video_file)
    else:
        st.info("íŒŒì¼ì„ ì„ íƒí•´ ì£¼ì„¸ìš”.")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4) ì„ ë°• ê´€ì œì •ë³´ ì¡°íšŒ ì„¹ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def vessel_monitoring_section():
    st.subheader("ğŸš¢ í•´ì–‘ìˆ˜ì‚°ë¶€ ì„ ë°• ê´€ì œì •ë³´ ì¡°íšŒ")
    date_from = st.date_input("ì¡°íšŒ ì‹œì‘ì¼", date.today(), key="vessel_from")
    date_to   = st.date_input("ì¡°íšŒ ì¢…ë£Œì¼", date.today(), key="vessel_to")
    page      = st.number_input("í˜ì´ì§€ ë²ˆí˜¸", 1, 1000, 1, key="vessel_page")
    per_page  = st.slider("í•œ ë²ˆì— ê°€ì ¸ì˜¬ ê±´ìˆ˜", 1, 1000, 100, key="vessel_perpage")
    if st.button("ğŸ” ì¡°íšŒ", key="vessel_search"):
        params = {
            "serviceKey": API_KEY,
            "page":       page,
            "perPage":    per_page,
            "fromDate":   date_from.strftime("%Y-%m-%d"),
            "toDate":     date_to.strftime("%Y-%m-%d"),
        }
        with st.spinner("ì¡°íšŒ ì¤‘â€¦"):
            res = requests.get(
                "https://api.odcloud.kr/api/15128156/v1/uddi:fdcdb0d1-0296-4c3b-8087-8ab4bd4d5123",
                params=params
            )
            if res.status_code != 200:
                st.error(f"API ì˜¤ë¥˜ {res.status_code}")
                st.text(res.text)
                return
            data = res.json()
        if data.get("data"):
            df = pd.DataFrame(data["data"])
            st.success(f"ì´ {data.get('totalCount', len(df))} ê±´ ì¡°íšŒë˜ì—ˆìŠµë‹ˆë‹¤.")
            st.dataframe(df)
        else:
            st.warning("ì¡°íšŒëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

def today_weather_section():
    st.subheader("â˜€ï¸ ì˜¤ëŠ˜ì˜ ë‚ ì”¨ ì¡°íšŒ")
    city_name = st.text_input("ë„ì‹œ ì´ë¦„ ì…ë ¥ (ì˜ˆ: ì„œìš¸, Seoul, ë¶€ì‚°, Busan)", key="weather_city_input")
    if st.button("ğŸ” ë‚ ì”¨ ê°€ì ¸ì˜¤ê¸°", key="weather_search"):
        if not city_name:
            st.warning("ë„ì‹œ ì´ë¦„ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
            return

        # í•œê¸€Â·ì˜ì–´ ëª¨ë‘ ì§€ì›í•˜ë„ë¡ URL-encode
        q_name = quote_plus(city_name)

        # í•œêµ­ì–´ ê²°ê³¼ ìš°ì„ ìœ¼ë¡œ language=ko ì¶”ê°€
        geo_url = (
            f"https://geocoding-api.open-meteo.com/v1/search?"
            f"name={q_name}&count=5&language=ko"
        )
        with st.spinner("ìœ„ì¹˜ ì •ë³´ ê²€ìƒ‰ ì¤‘â€¦"):
            geo_res = requests.get(geo_url)
        if geo_res.status_code != 200:
            st.error("ì§€ì˜¤ì½”ë”© API í˜¸ì¶œ ì‹¤íŒ¨")
            return

        geo_data = geo_res.json().get("results")
        if not geo_data:
            st.warning("í•´ë‹¹ ë„ì‹œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        loc = geo_data[0]
        lat  = loc["latitude"]
        lon  = loc["longitude"]
        display_name = f"{loc.get('name')}, {loc.get('country')}"

        # ë‚ ì”¨ ì •ë³´ ì¡°íšŒ
        weather_url = (
            f"https://api.open-meteo.com/v1/forecast?"
            f"latitude={lat}&longitude={lon}"
            f"&current_weather=true"
            f"&hourly=relativehumidity_2m"
            f"&timezone=auto"
        )
        with st.spinner(f"{display_name} ë‚ ì”¨ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘â€¦"):
            w_res = requests.get(weather_url)
        if w_res.status_code != 200:
            st.error("ë‚ ì”¨ API í˜¸ì¶œ ì‹¤íŒ¨")
            return

        js = w_res.json()
        cw = js.get("current_weather", {})
        temp, wind_spd, wind_dir, code = (
            cw.get("temperature"),
            cw.get("windspeed"),
            cw.get("winddirection"),
            cw.get("weathercode")
        )

        # ë‚ ì”¨ ì½”ë“œ ë§¤í•‘
        wc_map = {
            0:"ë§‘ìŒ",1:"ì£¼ë¡œ ë§‘ìŒ",2:"ë¶€ë¶„ì  êµ¬ë¦„",3:"êµ¬ë¦„ ë§ìŒ",
            45:"ì•ˆê°œ",48:"ì•ˆê°œ(ì…ìƒ)",
            51:"ì´ìŠ¬ë¹„ ì•½í•¨",53:"ì´ìŠ¬ë¹„ ë³´í†µ",55:"ì´ìŠ¬ë¹„ ê°•í•¨",
            61:"ë¹—ë°©ìš¸ ì•½í•¨",63:"ë¹—ë°©ìš¸ ë³´í†µ",65:"ë¹—ë°©ìš¸ ê°•í•¨",
            80:"ì†Œë‚˜ê¸° ì•½í•¨",81:"ì†Œë‚˜ê¸° ë³´í†µ",82:"ì†Œë‚˜ê¸° ê°•í•¨",
            95:"ë‡Œìš°",96:"ì•½í•œ ë‡Œìš°",99:"ê°•í•œ ë‡Œìš°"
        }
        desc = wc_map.get(code, "ì•Œ ìˆ˜ ì—†ìŒ")

        # ìŠµë„ ì–»ê¸°
        times = js.get("hourly", {}).get("time", [])
        hums  = js.get("hourly", {}).get("relativehumidity_2m", [])
        now   = datetime.now().strftime("%Y-%m-%dT%H:00")
        humidity = hums[times.index(now)] if now in times else None

        # ê²°ê³¼ ì¶œë ¥
        st.markdown(f"### {display_name} í˜„ì¬ ë‚ ì”¨")
        c1, c2, c3, c4 = st.columns(4)
        c1.metric("ğŸŒ¡ï¸ ê¸°ì˜¨(â„ƒ)", temp)
        c2.metric("ğŸ’¨ í’ì†(m/s)", wind_spd)
        c3.metric("ğŸŒ«ï¸ í’í–¥(Â°)", wind_dir)
        c4.metric("ğŸ’§ ìŠµë„(%)", humidity if humidity is not None else "â€“")
        st.markdown(f"**ë‚ ì”¨ ìƒíƒœ:** {desc}")
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 6) LLM í…ŒìŠ¤íŠ¸ (Hugging Face Inference API) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€) LLM í…ŒìŠ¤íŠ¸ (Hugging Face) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource
def generate_with_hf(prompt: str) -> str:
    if not HF_API_TOKEN:
        return "âš ï¸ HF_API_TOKEN í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    headers = {"Authorization": f"Bearer {HF_API_TOKEN}", "Content-Type": "application/json"}
    payload = {"inputs": prompt, "parameters": {"max_new_tokens": 256}}
    try:
        r = requests.post(HF_API_URL, headers=headers, json=payload, timeout=30)
        r.raise_for_status()
        data = r.json()
        if isinstance(data, dict) and "generated_text" in data:
            return data["generated_text"]
        if isinstance(data, list) and data and "generated_text" in data[0]:
            return data[0]["generated_text"]
        return str(data)
    except Exception as e:
        return f"HF í˜¸ì¶œ ì˜¤ë¥˜: {e}"

def llm_section():
    st.subheader("ğŸ¤– LLM í…ŒìŠ¤íŠ¸ (Hugging Face Inference API)")
    prompt = st.text_area("í”„ë¡¬í”„íŠ¸ ì…ë ¥", height=150, key="llm_prompt")
    if st.button("ìƒì„±", key="hf_generate"):
        with st.spinner("API í˜¸ì¶œ ì¤‘â€¦"):
            out = generate_with_hf(prompt)
        st.markdown("### ì‘ë‹µ")
        st.write(out)
    st.info("âš™ï¸ ì‚¬ìš© ì „ HF_API_TOKENê³¼ HF_API_URLì„ Secretsì— ì„¤ì •í•´ì£¼ì„¸ìš”.")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 7) ì•± ë ˆì´ì•„ì›ƒ (íƒ­ êµ¬ì„±) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="í†µí•© ë°ëª¨", layout="centered")
st.title("ğŸ“ˆ í†µí•© ë°ëª¨: ë‰´ìŠ¤Â·ë°ì´í„°Â·ë™ì˜ìƒÂ·ì„ ë°•Â·ë‚ ì”¨Â·LLM")

tabs = st.tabs(["êµ¬ê¸€ ë‰´ìŠ¤", "ë°ì´í„° íˆìŠ¤í† ê·¸ë¨", "ë™ì˜ìƒ ì¬ìƒ", "ì„ ë°• ê´€ì œì •ë³´", "ì˜¤ëŠ˜ì˜ ë‚ ì”¨", "LLM í…ŒìŠ¤íŠ¸"])
with tabs[0]:
    st.subheader("â–¶ êµ¬ê¸€ ë‰´ìŠ¤ í¬ë¡¤ë§ (RSS)")
    kw = st.text_input("ê²€ìƒ‰ í‚¤ì›Œë“œ", "ESG", key="news_kw")
    num = st.slider("ê°€ì ¸ì˜¬ ê¸°ì‚¬ ê°œìˆ˜", 5, 20, 10, key="news_num")
    if st.button("ë³´ê¸°", key="news_btn"):
        with st.spinner(f"â€˜{kw}â€™ ë‰´ìŠ¤ ë¡œë”©â€¦"):
            for it in fetch_google_news(kw, num):
                st.markdown(f"- **[{it['source']} Â· {it['date']}]** [{it['title']}]({it['link']})")
with tabs[1]:
    sample_data_section()
with tabs[2]:
    video_upload_section()
with tabs[3]:
    vessel_monitoring_section()
with tabs[4]:
    today_weather_section()
with tabs[5]:
    llm_section()




