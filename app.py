import os
import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import feedparser
import requests
from datetime import datetime, date
from urllib.parse import urlencode, quote_plus
from dotenv import load_dotenv
from llama_index.core import (
    VectorStoreIndex, SimpleDirectoryReader,
    StorageContext, load_index_from_storage, Settings
)
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.llms.openai import OpenAI
import logging, traceback
import openai                             # â† NEW
openai.api_key = (
    st.secrets.get("OPENAI_API_KEY")         # .streamlit/secrets.toml
    or os.getenv("OPENAI_API_KEY", "")       # í™˜ê²½ë³€ìˆ˜
)

# API í‚¤ë“¤ (ëª¨ë‘ Secrets ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤)
API_KEY      = os.getenv("ODCLOUD_API_KEY")
HF_API_TOKEN = os.getenv("HF_API_TOKEN")
HF_API_URL   = os.getenv("HF_API_URL")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1) ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ (Google News RSS) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data(ttl=300)
def fetch_google_news(keyword: str, max_items: int = 10):
    clean_kw = " ".join(keyword.strip().split())
    params = {"q": clean_kw, "hl": "ko", "gl": "KR", "ceid": "KR:ko"}
    rss_url = "https://news.google.com/rss/search?" + urlencode(params, doseq=True)
    feed = feedparser.parse(rss_url)
    items = []
    for entry in feed.entries[:max_items]:
        pub_date = datetime(*entry.published_parsed[:6]).strftime("%Y-%m-%d")
        source   = entry.get("source", {}).get("title", "")
        items.append({
            "title":  entry.title,
            "link":   entry.link,
            "source": source,
            "date":   pub_date,
        })
    return items

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2) CSV íˆìŠ¤í† ê·¸ë¨ ì„¹ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def sample_data_section():
    st.subheader("ğŸ“Š ìƒ˜í”Œ ë°ì´í„° íˆìŠ¤í† ê·¸ë¨")
    uploaded_file = st.file_uploader("CSV íŒŒì¼ ì—…ë¡œë“œ (optional)", type=["csv"])
    if uploaded_file:
        df = pd.read_csv(uploaded_file)
        st.dataframe(df)
        nums = df.select_dtypes(include="number").columns.tolist()
        if nums:
            col = st.selectbox("Numeric ì»¬ëŸ¼ ì„ íƒ", nums)
            fig, ax = plt.subplots()
            ax.hist(df[col], bins=10)
            ax.set_xlabel(col)
            ax.set_ylabel("Frequency")
            st.pyplot(fig)
    else:
        st.info("CSV íŒŒì¼ì„ ì˜¬ë¦¬ë©´ íˆìŠ¤í† ê·¸ë¨ì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3) ë™ì˜ìƒ ì—…ë¡œë“œÂ·ì¬ìƒ ì„¹ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def video_upload_section():
    st.subheader("ğŸ“¹ ë™ì˜ìƒ ì—…ë¡œë“œ & ì¬ìƒ")
    video_file = st.file_uploader("ë™ì˜ìƒ íŒŒì¼ ì—…ë¡œë“œ", type=["mp4","mov","avi"])
    if video_file:
        st.video(video_file)
    else:
        st.info("íŒŒì¼ì„ ì„ íƒí•´ ì£¼ì„¸ìš”.")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4) ì„ ë°• ê´€ì œì •ë³´ ì¡°íšŒ ì„¹ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def vessel_monitoring_section():
    st.subheader("ğŸš¢ í•´ì–‘ìˆ˜ì‚°ë¶€ ì„ ë°• ê´€ì œì •ë³´ ì¡°íšŒ")
    date_from = st.date_input("ì¡°íšŒ ì‹œì‘ì¼", date.today())
    date_to   = st.date_input("ì¡°íšŒ ì¢…ë£Œì¼", date.today())
    page      = st.number_input("í˜ì´ì§€ ë²ˆí˜¸", 1, 1000, 1)
    per_page  = st.slider("í•œ ë²ˆì— ê°€ì ¸ì˜¬ ê±´ìˆ˜", 1, 1000, 100)
    if st.button("ğŸ” ì¡°íšŒ"):
        params = {
            "serviceKey": API_KEY,
            "page":       page,
            "perPage":    per_page,
            "fromDate":   date_from.strftime("%Y-%m-%d"),
            "toDate":     date_to.strftime("%Y-%m-%d"),
        }
        with st.spinner("ì¡°íšŒ ì¤‘â€¦"):
            res = requests.get(
                "https://api.odcloud.kr/api/15128156/v1/uddi:fdcdb0d1-0296-4c3b-8087-8ab4bd4d5123",
                params=params
            )
        if res.status_code != 200:
            st.error(f"API ì˜¤ë¥˜ {res.status_code}")
            return
        data = res.json().get("data", [])
        if data:
            df = pd.DataFrame(data)
            st.success(f"ì´ {len(df)} ê±´ ì¡°íšŒë˜ì—ˆìŠµë‹ˆë‹¤.")
            st.dataframe(df)
        else:
            st.warning("ì¡°íšŒëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 5) ì˜¤ëŠ˜ì˜ ë‚ ì”¨ ì„¹ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def today_weather_section():
    st.subheader("â˜€ï¸ ì˜¤ëŠ˜ì˜ ë‚ ì”¨ ì¡°íšŒ")
    city_name = st.text_input("ë„ì‹œ ì´ë¦„ ì…ë ¥ (ì˜ˆ: ì„œìš¸, Busan)")
    if st.button("ğŸ” ë‚ ì”¨ ê°€ì ¸ì˜¤ê¸°"):
        if not city_name:
            st.warning("ë„ì‹œ ì´ë¦„ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
            return

        q_name  = quote_plus(city_name)
        geo_url = f"https://geocoding-api.open-meteo.com/v1/search?name={q_name}&count=5&language=ko"
        with st.spinner("ìœ„ì¹˜ ê²€ìƒ‰ ì¤‘â€¦"):
            geo_res = requests.get(geo_url)
        if geo_res.status_code != 200:
            st.error("ì§€ì˜¤ì½”ë”© API ì˜¤ë¥˜")
            return
        results = geo_res.json().get("results")
        if not results:
            st.warning("ë„ì‹œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        loc  = results[0]
        lat  = loc["latitude"]
        lon  = loc["longitude"]
        display_name = f"{loc['name']}, {loc['country']}"

        weather_url = (
            f"https://api.open-meteo.com/v1/forecast?"
            f"latitude={lat}&longitude={lon}&current_weather=true"
            f"&hourly=relativehumidity_2m&timezone=auto"
        )
        with st.spinner(f"{display_name} ë‚ ì”¨ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘â€¦"):
            w_res = requests.get(weather_url)
        if w_res.status_code != 200:
            st.error("ë‚ ì”¨ API ì˜¤ë¥˜")
            return

        js   = w_res.json()
        cw   = js.get("current_weather", {})
        temp, wind_spd, wind_dir, code = (
            cw.get("temperature"),
            cw.get("windspeed"),
            cw.get("winddirection"),
            cw.get("weathercode"),
        )
        wc_map = {
            0:"ë§‘ìŒ",1:"ì£¼ë¡œ ë§‘ìŒ",2:"ë¶€ë¶„ì  êµ¬ë¦„",3:"êµ¬ë¦„ ë§ìŒ",
            45:"ì•ˆê°œ",48:"ì•ˆê°œ(ì…ìƒ)",
            51:"ì´ìŠ¬ë¹„ ì•½í•¨",53:"ì´ìŠ¬ë¹„ ë³´í†µ",55:"ì´ìŠ¬ë¹„ ê°•í•¨",
            61:"ë¹—ë°©ìš¸ ì•½í•¨",63:"ë¹—ë°©ìš¸ ë³´í†µ",65:"ë¹—ë°©ìš¸ ê°•í•¨",
            80:"ì†Œë‚˜ê¸° ì•½í•¨",81:"ì†Œë‚˜ê¸° ë³´í†µ",82:"ì†Œë‚˜ê¸° ê°•í•¨",
            95:"ë‡Œìš°",96:"ì•½í•œ ë‡Œìš°",99:"ê°•í•œ ë‡Œìš°"
        }
        desc      = wc_map.get(code, "ì•Œ ìˆ˜ ì—†ìŒ")
        times     = js["hourly"]["time"]
        hums      = js["hourly"]["relativehumidity_2m"]
        now       = datetime.now().strftime("%Y-%m-%dT%H:00")
        humidity  = hums[times.index(now)] if now in times else None

        st.markdown(f"### {display_name} í˜„ì¬ ë‚ ì”¨")
        c1, c2, c3, c4 = st.columns(4)
        c1.metric("ğŸŒ¡ï¸ ê¸°ì˜¨(â„ƒ)", temp)
        c2.metric("ğŸ’¨ í’ì†(m/s)", wind_spd)
        c3.metric("ğŸŒ«ï¸ í’í–¥(Â°)", wind_dir)
        c4.metric("ğŸ’§ ìŠµë„(%)", humidity or "â€“")
        st.markdown(f"**ë‚ ì”¨ ìƒíƒœ:** {desc}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 6) LLM í…ŒìŠ¤íŠ¸ (Hugging Face Inference API) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def generate_with_kanana(prompt: str) -> str:
    headers = {"Authorization": f"Bearer {HF_API_TOKEN}"}
    payload = {
        "inputs":     prompt,
        "options":    {"use_cache": False},
        "parameters": {"max_new_tokens": 150, "temperature": 0.7}
    }
    res = requests.post(HF_API_URL, headers=headers, json=payload, timeout=30)
    res.raise_for_status()
    return res.json()[0]["generated_text"]

def llm_section():
    st.subheader("ğŸ¤– ì¹´ë‚˜ë‚˜ Nano (Hugging Face Inference API)")
    prompt = st.text_area("í”„ë¡¬í”„íŠ¸ ì…ë ¥", height=150)
    if st.button("ìƒì„±"):
        if not HF_API_TOKEN or not HF_API_URL:
            st.error("HF_API_TOKEN ë˜ëŠ” HF_API_URLì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        with st.spinner("ì‘ë‹µ ìƒì„± ì¤‘â€¦"):
            try:
                out = generate_with_kanana(prompt)
                st.markdown("### ì‘ë‹µ")
                st.write(out)
            except Exception as e:
                st.error(f"LLM í˜¸ì¶œ ì˜¤ë¥˜: {e}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 7) ë¬¸ì„œ ê¸°ë°˜ ì±—ë´‡ (LlamaIndex) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def rag_chatbot_section():
    st.subheader("ğŸ“š ë¬¸ì„œ ê¸°ë°˜ ì±—ë´‡ (RAG with LlamaIndex)")

    # â”€â”€ ì‚¬ì´ë“œë°” (í‚¤â€§íŒŒì¼ ì—…ë¡œë“œ)
    with st.sidebar:
        st.markdown("### ğŸ”‘ OpenAI API Key")
        api_key = st.text_input(
            "OPENAI_API_KEY",
            value=st.secrets.get("OPENAI_API_KEY", ""),
            type="password"
        )
        uploaded_file = st.file_uploader(
            "ğŸ“„ ì¸ë±ì‹±í•  ë¬¸ì„œ ì—…ë¡œë“œ",
            type=["txt", "pdf", "md", "docx", "pptx", "csv"]
        )

    # â”€â”€ ì„¸ì…˜ ì´ˆê¸°í™” & ë””ë ‰í„°ë¦¬ ì¤€ë¹„
    if "rag_messages" not in st.session_state:
        st.session_state.rag_messages = []
    if "chat_engine" not in st.session_state:
        st.session_state.chat_engine = None

    os.makedirs("./cache/data", exist_ok=True)
    os.makedirs("./storage",    exist_ok=True)

    # â”€â”€ íŒŒì¼ ì—…ë¡œë“œ í•¸ë“¤ë§
    if uploaded_file is not None:
        file_path = os.path.join("cache", "data", uploaded_file.name)
        with open(file_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        st.success(f"â€˜{uploaded_file.name}â€™ ì—…ë¡œë“œ ì™„ë£Œ!")

    # â”€â”€ LlamaIndex ì„¸íŒ…
    if api_key:
        load_dotenv()
        Settings.llm = OpenAI(model="gpt-4o-mini", temperature=0, api_key=api_key)
        Settings.embed_model = OpenAIEmbedding(
            model="text-embedding-3-small",
            api_key=api_key
        )
    else:
        st.warning("ğŸ”‘ OpenAI API Keyë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
        return  # <-- ì—¬ê¸°ì„œ í•¨ìˆ˜ë§Œ ë¹ ì ¸ë‚˜ê°€ë„ë¡ ë³€ê²½

    # â”€â”€ ì¸ë±ìŠ¤ ìƒì„±Â·ë¡œë“œ (ìºì‹œ í™œìš©)
    @st.cache_resource(show_spinner="ğŸ”§ ì¸ë±ìŠ¤ ë¹Œë“œ ì¤‘â€¦")
    def load_or_build_index() -> VectorStoreIndex:
        persist_dir = "./storage"
        if os.listdir("cache/data"):
            docs = SimpleDirectoryReader("cache/data").load_data()
            idx  = VectorStoreIndex.from_documents(docs)
            idx.storage_context.persist(persist_dir)
            return idx
        if os.path.exists(os.path.join(persist_dir, "docstore.json")):
            sc  = StorageContext.from_defaults(persist_dir=persist_dir)
            return load_index_from_storage(sc)
        return None

    index = load_or_build_index()
    if index is None:
        st.info("ë¨¼ì € ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê±°ë‚˜ storage í´ë”ì— ê¸°ì¡´ ì¸ë±ìŠ¤ë¥¼ ë‘ì„¸ìš”.")
        return  # <-- ì—¬ê¸°ì„œë„ í•¨ìˆ˜ë§Œ ë¹ ì ¸ë‚˜ê°€ë„ë¡

    # â”€â”€ ChatEngine ì¤€ë¹„ (ìŠ¤íŠ¸ë¦¬ë°)
    if st.session_state.chat_engine is None:
        st.session_state.chat_engine = index.as_chat_engine(
            chat_mode="context",
            similarity_top_k=4,
            streaming=True
        )

    # â”€â”€ ì´ì „ ëŒ€í™” ë Œë”ë§
    for msg in st.session_state.rag_messages:
        st.chat_message(msg["role"]).markdown(msg["content"])

    # â”€â”€ ì‚¬ìš©ì ì…ë ¥ & ì‘ë‹µ
    user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.", key="rag_input")
    if user_input:
        st.session_state.rag_messages.append({"role": "user", "content": user_input})
        st.chat_message("user").markdown(user_input)

        try:
            with st.chat_message("assistant"):
                stream_resp = st.session_state.chat_engine.stream_chat(user_input)
                buffer = ""
                for chunk in stream_resp.response_gen:
                    buffer += chunk
                    st.write(buffer + "â–Œ")
                st.session_state.rag_messages.append(
                    {"role": "assistant", "content": buffer}
                )
        except Exception as e:
            st.error(f"âš ï¸ ì˜¤ë¥˜: {e}")
            traceback.print_exc()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 6-B) ChatGPT í´ë¡  ì„¹ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def chatgpt_clone_section():
    st.subheader("ğŸ’¬ ChatGPT í´ë¡  (OpenAI ChatCompletion)")

    # â‘  ì„¸ì…˜ ìƒíƒœ ì¤€ë¹„
    if "gpt_messages" not in st.session_state:
        st.session_state.gpt_messages = []

    # â‘¡ ì´ì „ ëŒ€í™” ì¶œë ¥
    for m in st.session_state.gpt_messages:
        with st.chat_message(m["role"]):
            st.markdown(m["content"])

    # â‘¢ ì‚¬ìš©ì ì…ë ¥ (ê³ ìœ  key ì§€ì •)
    prompt = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”", key="gpt_clone_input")
    if not prompt:
        return

    # â‘£ ìœ ì € ë©”ì‹œì§€ ê¸°ë¡ ë° ë Œë”ë§
    st.session_state.gpt_messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # â‘¤ OpenAI ëŒ€í™” ì™„ì„±(ìŠ¤íŠ¸ë¦¬ë°)
    try:
        resp = openai.ChatCompletion.create(
            model="gpt-4o-mini",              # í•„ìš”ì‹œ gpt-3.5-turbo ë¡œ ë³€ê²½
            messages=st.session_state.gpt_messages,
            stream=True
        )

        assistant_buf = ""
        with st.chat_message("assistant"):
            placeholder = st.empty()
            for chunk in resp:
                delta = chunk["choices"][0].get("delta", {})
                if "content" in delta:
                    assistant_buf += delta["content"]
                    placeholder.markdown(assistant_buf + "â–Œ")
            placeholder.markdown(assistant_buf)

        # â‘¥ ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ê¸°ë¡
        st.session_state.gpt_messages.append(
            {"role": "assistant", "content": assistant_buf}
        )

    except Exception as e:
        st.error(f"OpenAI í˜¸ì¶œ ì˜¤ë¥˜: {e}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 7) ì•± ë ˆì´ì•„ì›ƒ (íƒ­ êµ¬ì„±) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="í†µí•© ë°ëª¨", layout="centered")
st.title("ğŸ“ˆ í†µí•© ë°ëª¨: ë‰´ìŠ¤Â·ë°ì´í„°Â·ë™ì˜ìƒÂ·ì„ ë°•Â·ë‚ ì”¨Â·LLM")

tabs = st.tabs([
    "êµ¬ê¸€ ë‰´ìŠ¤", "ë°ì´í„° íˆìŠ¤í† ê·¸ë¨", "ë™ì˜ìƒ ì¬ìƒ",
    "ì„ ë°• ê´€ì œì •ë³´", "ì˜¤ëŠ˜ì˜ ë‚ ì”¨", "LLM í…ŒìŠ¤íŠ¸","ë¬¸ì„œ ì±—ë´‡" , "ChatGPT í´ë¡ " 
   
])
with tabs[0]:
    st.subheader("â–¶ êµ¬ê¸€ ë‰´ìŠ¤ í¬ë¡¤ë§ (RSS)")
    kw  = st.text_input("ê²€ìƒ‰ í‚¤ì›Œë“œ", "ESG")
    num = st.slider("ê°€ì ¸ì˜¬ ê¸°ì‚¬ ê°œìˆ˜", 5, 50 , 10)
    if st.button("ë³´ê¸°"):
        for it in fetch_google_news(kw, num):
            st.markdown(f"- **[{it['source']} Â· {it['date']}]** [{it['title']}]({it['link']})")
with tabs[1]:
    sample_data_section()
with tabs[2]:
    video_upload_section()
with tabs[3]:
    vessel_monitoring_section()
with tabs[4]:
    today_weather_section()
with tabs[5]:
    llm_section()

with tabs[6]:
    rag_chatbot_section()

with tabs[7]:           # ìƒˆ íƒ­ ì¸ë±ìŠ¤
    chatgpt_clone_section()





