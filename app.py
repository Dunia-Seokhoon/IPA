# app.py  |  Streamlit í†µí•© ë°ëª¨ (HF Llama3 PDF ì±—ë´‡)

import os
from datetime import datetime, date
from urllib.parse import urlencode, quote_plus

import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import feedparser
import requests

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ LangChain Ã— Hugging Face â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from langchain_huggingface import (
    ChatHuggingFace,
    HuggingFaceHubEmbeddings,
)
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain.chains import ConversationalRetrievalChain

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Secrets â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
API_KEY    = os.getenv("ODCLOUD_API_KEY")             # í•´ì–‘ìˆ˜ì‚°ë¶€ ì˜¤í”ˆë°ì´í„°
HF_API_TOKEN = os.getenv("HF_API_TOKEN")              # GPT-2 í…ŒìŠ¤íŠ¸ìš©
HFHUB_TOKEN  = os.getenv("HUGGINGFACEHUB_API_TOKEN")  # Llama3Â·ì„ë² ë”©ìš©

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1) êµ¬ê¸€ ë‰´ìŠ¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data(ttl=300)
def fetch_google_news(keyword: str, max_items: int = 10):
    params  = {"q": keyword, "hl": "ko", "gl": "KR", "ceid": "KR:ko"}
    rss_url = "https://news.google.com/rss/search?" + urlencode(params, doseq=True)
    feed    = feedparser.parse(rss_url)
    items   = []
    for e in feed.entries[:max_items]:
        d = datetime(*e.published_parsed[:6]).strftime("%Y-%m-%d")
        items.append({"title": e.title, "link": e.link,
                      "source": e.get("source", {}).get("title", ""), "date": d})
    return items

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2) CSV íˆìŠ¤í† ê·¸ë¨ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def sample_data_section():
    st.subheader("ğŸ“Š ìƒ˜í”Œ ë°ì´í„° íˆìŠ¤í† ê·¸ë¨")
    upl = st.file_uploader("CSV íŒŒì¼ ì—…ë¡œë“œ (optional)", type=["csv"])
    if not upl:
        st.info("CSV íŒŒì¼ì„ ì˜¬ë¦¬ë©´ íˆìŠ¤í† ê·¸ë¨ì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return
    df = pd.read_csv(upl)
    st.dataframe(df)
    nums = df.select_dtypes(include="number").columns.tolist()
    if not nums:
        st.warning("ìˆ«ìí˜• ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    col = st.selectbox("Numeric ì»¬ëŸ¼ ì„ íƒ", nums)
    fig, ax = plt.subplots()
    ax.hist(df[col], bins=10)
    ax.set_xlabel(col)
    ax.set_ylabel("Frequency")
    st.pyplot(fig)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3) ë™ì˜ìƒ ì¬ìƒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def video_upload_section():
    st.subheader("ğŸ“¹ ë™ì˜ìƒ ì—…ë¡œë“œ & ì¬ìƒ")
    vfile = st.file_uploader("ë™ì˜ìƒ íŒŒì¼ ì—…ë¡œë“œ", type=["mp4", "mov", "avi"])
    st.video(vfile) if vfile else st.info("íŒŒì¼ì„ ì„ íƒí•´ ì£¼ì„¸ìš”.")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4) ì„ ë°• ê´€ì œì •ë³´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def vessel_monitoring_section():
    st.subheader("ğŸš¢ í•´ì–‘ìˆ˜ì‚°ë¶€ ì„ ë°• ê´€ì œì •ë³´ ì¡°íšŒ")
    d_from = st.date_input("ì¡°íšŒ ì‹œì‘ì¼", date.today())
    d_to   = st.date_input("ì¡°íšŒ ì¢…ë£Œì¼", date.today())
    page   = st.number_input("í˜ì´ì§€ ë²ˆí˜¸", 1, 1000, 1)
    per    = st.slider("í•œ ë²ˆì— ê°€ì ¸ì˜¬ ê±´ìˆ˜", 1, 1000, 100)
    if not st.button("ğŸ” ì¡°íšŒ"):
        return
    params = {
        "serviceKey": API_KEY, "page": page, "perPage": per,
        "fromDate": d_from.strftime("%Y-%m-%d"), "toDate": d_to.strftime("%Y-%m-%d"),
    }
    with st.spinner("ì¡°íšŒ ì¤‘â€¦"):
        r = requests.get(
            "https://api.odcloud.kr/api/15128156/v1/uddi:fdcdb0d1-0296-4c3b-8087-8ab4bd4d5123",
            params=params)
    if r.status_code != 200:
        st.error(f"API ì˜¤ë¥˜ {r.status_code}")
        return
    data = r.json().get("data", [])
    if data:
        st.success(f"ì´ {len(data)} ê±´ ì¡°íšŒë˜ì—ˆìŠµë‹ˆë‹¤.")
        st.dataframe(pd.DataFrame(data))
    else:
        st.warning("ì¡°íšŒëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 5) ì˜¤ëŠ˜ì˜ ë‚ ì”¨ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def today_weather_section():
    st.subheader("â˜€ï¸ ì˜¤ëŠ˜ì˜ ë‚ ì”¨ ì¡°íšŒ")
    city = st.text_input("ë„ì‹œ ì´ë¦„ ì…ë ¥ (ì˜ˆ: ì„œìš¸, Busan)")
    if not st.button("ğŸ” ë‚ ì”¨ ê°€ì ¸ì˜¤ê¸°"):
        return
    if not city:
        st.warning("ë„ì‹œ ì´ë¦„ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”."); return
    q     = quote_plus(city)
    geo_r = requests.get(
        f"https://geocoding-api.open-meteo.com/v1/search?name={q}&count=5&language=ko")
    if geo_r.status_code != 200:
        st.error("ì§€ì˜¤ì½”ë”© API ì˜¤ë¥˜"); return
    j = geo_r.json().get("results")
    if not j:
        st.warning("ë„ì‹œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."); return
    loc = j[0]; lat, lon = loc["latitude"], loc["longitude"]
    w_r = requests.get(
        f"https://api.open-meteo.com/v1/forecast?"
        f"latitude={lat}&longitude={lon}&current_weather=true"
        f"&hourly=relativehumidity_2m&timezone=auto")
    if w_r.status_code != 200:
        st.error("ë‚ ì”¨ API ì˜¤ë¥˜"); return
    cw  = w_r.json()["current_weather"]
    wc  = {0:"ë§‘ìŒ",1:"ì£¼ë¡œ ë§‘ìŒ",2:"ë¶€ë¶„ì  êµ¬ë¦„",3:"êµ¬ë¦„ ë§ìŒ",
           45:"ì•ˆê°œ",48:"ì•ˆê°œ(ì…ìƒ)",51:"ì´ìŠ¬ë¹„ ì•½í•¨",53:"ì´ìŠ¬ë¹„ ë³´í†µ",
           55:"ì´ìŠ¬ë¹„ ê°•í•¨",61:"ë¹—ë°©ìš¸ ì•½í•¨",63:"ë¹—ë°©ìš¸ ë³´í†µ",65:"ë¹—ë°©ìš¸ ê°•í•¨",
           80:"ì†Œë‚˜ê¸° ì•½í•¨",81:"ì†Œë‚˜ê¸° ë³´í†µ",82:"ì†Œë‚˜ê¸° ê°•í•¨",
           95:"ë‡Œìš°",96:"ì•½í•œ ë‡Œìš°",99:"ê°•í•œ ë‡Œìš°"}
    st.markdown(f"### {loc['name']}, {loc['country']} í˜„ì¬ ë‚ ì”¨")
    c1,c2,c3 = st.columns(3)
    c1.metric("ğŸŒ¡ï¸ ê¸°ì˜¨(â„ƒ)", cw["temperature"])
    c2.metric("ğŸ’¨ í’ì†(m/s)", cw["windspeed"])
    c3.metric("ìƒíƒœ", wc.get(cw["weathercode"], "ì•Œ ìˆ˜ ì—†ìŒ"))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 6) GPT-2 í…ŒìŠ¤íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def generate_with_gpt2(prompt: str) -> str:
    headers = {"Authorization": f"Bearer {HF_API_TOKEN}"}
    r = requests.post(
        "https://api-inference.huggingface.co/models/gpt2",
        headers=headers,
        json={"inputs":prompt,"parameters":{"max_new_tokens":150}},
        timeout=30)
    r.raise_for_status()
    return r.json()[0]["generated_text"]

def llm_section():
    st.subheader("ğŸ¤– GPT-2 í…ŒìŠ¤íŠ¸ (HF Inference API)")
    p = st.text_area("í”„ë¡¬í”„íŠ¸ ì…ë ¥", height=150)
    if st.button("ìƒì„±"):
        try:
            with st.spinner("ìƒì„± ì¤‘â€¦"):
                st.write(generate_with_gpt2(p))
        except Exception as e:
            st.error(e)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 7) PDF ì±—ë´‡ (HF Llama-3 8B) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def pdf_chatbot_section():
    st.subheader("ğŸ“‘ PDF ì±—ë´‡ (Llama-3 8B + MiniLM)")
    pdf = st.file_uploader("PDF ì—…ë¡œë“œ", type=["pdf"])
    if "hist" not in st.session_state:
        st.session_state.hist = []
    if not pdf:
        st.info("PDF íŒŒì¼ì„ ì˜¬ë¦¬ë©´ ì§ˆë¬¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."); return

    # 1) ë¬¸ì„œ ë¡œë“œ & ë¶„í• 
    docs = PyPDFLoader(pdf).load_and_split()

    # 2) ì„ë² ë”© & ë²¡í„°ìŠ¤í† ì–´
    embed = HuggingFaceHubEmbeddings(
        model_name             ="sentence-transformers/all-MiniLM-L6-v2",
        huggingfacehub_api_token=HFHUB_TOKEN)
    store = FAISS.from_documents(docs, embed)

    # 3) ì±— ëª¨ë¸
    model = ChatHuggingFace(
        repo_id                ="meta-llama/Meta-Llama-3-8B-Instruct",
        huggingfacehub_api_token=HFHUB_TOKEN,
        temperature            =0.2)

    chain = ConversationalRetrievalChain.from_llm(
        llm       =model,
        retriever =store.as_retriever(),
        return_source_documents=True)

    q = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")
    if st.button("ì§ˆë¬¸í•˜ê¸°") and q:
        with st.spinner("ë‹µë³€ ìƒì„± ì¤‘â€¦"):
            res = chain({"question":q,"chat_history":st.session_state.hist})
        ans = res["answer"]
        st.session_state.hist.append((q, ans))
        st.markdown("### ğŸ’¬ ë‹µë³€")
        st.write(ans)
        with st.expander("ğŸ” ì°¸ì¡° ë¬¸ì„œ"):
            for d in res["source_documents"]:
                pnum = d.metadata.get("page","?")
                st.markdown(f"- p.{pnum}: {d.page_content[:120]}â€¦")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 8) í˜ì´ì§€ ë ˆì´ì•„ì›ƒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="í†µí•© ë°ëª¨", layout="centered")
st.title("ğŸ“ˆ í†µí•© ë°ëª¨: ë‰´ìŠ¤Â·ë°ì´í„°Â·ë™ì˜ìƒÂ·ì„ ë°•Â·ë‚ ì”¨Â·LLMÂ·PDF ì±—ë´‡")

tabs = st.tabs([
    "êµ¬ê¸€ ë‰´ìŠ¤", "ë°ì´í„° íˆìŠ¤í† ê·¸ë¨", "ë™ì˜ìƒ ì¬ìƒ",
    "ì„ ë°• ê´€ì œì •ë³´", "ì˜¤ëŠ˜ì˜ ë‚ ì”¨", "LLM í…ŒìŠ¤íŠ¸", "PDF ì±—ë´‡"
])

with tabs[0]:  # ë‰´ìŠ¤
    st.subheader("â–¶ êµ¬ê¸€ ë‰´ìŠ¤ (RSS)")
    kw  = st.text_input("ê²€ìƒ‰ í‚¤ì›Œë“œ", "ESG")
    num = st.slider("ê°€ì ¸ì˜¬ ê¸°ì‚¬ ê°œìˆ˜", 5, 20, 10)
    if st.button("ë‰´ìŠ¤ ë³´ê¸°"):
        for it in fetch_google_news(kw, num):
            st.markdown(f"- **[{it['source']} Â· {it['date']}]** [{it['title']}]({it['link']})")

with tabs[1]: sample_data_section()
with tabs[2]: video_upload_section()
with tabs[3]: vessel_monitoring_section()
with tabs[4]: today_weather_section()
with tabs[5]: llm_section()
with tabs[6]: pdf_chatbot_section()





