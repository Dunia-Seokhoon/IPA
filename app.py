import os
import streamlit as st
import streamlit.components.v1 as components
import openai
import base64
import backoff
import tiktoken
import time
import pandas as pd
import matplotlib.pyplot as plt
import feedparser
import requests
from datetime import datetime, date
from urllib.parse import urlencode, quote_plus
from dotenv import load_dotenv
from llama_index.core import (
    VectorStoreIndex, SimpleDirectoryReader,
    StorageContext, load_index_from_storage, Settings
)
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.llms.openai import OpenAI
import logging, traceback
from io import BytesIO
from PIL import Image

load_dotenv()

# â”€â”€â”€ API í‚¤ë“¤ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
openai.api_key = (
    st.secrets.get("OPENAI_API_KEY")
    or os.getenv("OPENAI_API_KEY", "")
)
API_KEY      = os.getenv("ODCLOUD_API_KEY")
HF_API_TOKEN = os.getenv("HF_API_TOKEN")
HF_API_URL   = os.getenv("HF_API_URL")

# â”€â”€â”€ 1) ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ (Google News RSS) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data(ttl=300)
def fetch_google_news(keyword: str, max_items: int = 10):
    clean_kw = " ".join(keyword.strip().split())
    params = {"q": clean_kw, "hl": "ko", "gl": "KR", "ceid": "KR:ko"}
    rss_url = "https://news.google.com/rss/search?" + urlencode(params, doseq=True)
    feed = feedparser.parse(rss_url)
    items = []
    for entry in feed.entries[:max_items]:
        pub_date = datetime(*entry.published_parsed[:6]).strftime("%Y-%m-%d")
        source   = entry.get("source", {}).get("title", "")
        items.append({
            "title":  entry.title,
            "link":   entry.link,
            "source": source,
            "date":   pub_date,
        })
    return items

# â”€â”€â”€ 2) ì„ ë°• ê´€ì œì •ë³´ ì¡°íšŒ ì„¹ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def vessel_monitoring_section():
    st.subheader("ğŸš¢ í•´ì–‘ìˆ˜ì‚°ë¶€ ì„ ë°• ê´€ì œì •ë³´ ì¡°íšŒ")
    date_from = st.date_input("ì¡°íšŒ ì‹œì‘ì¼", date.today())
    date_to   = st.date_input("ì¡°íšŒ ì¢…ë£Œì¼", date.today())
    page      = st.number_input("í˜ì´ì§€ ë²ˆí˜¸", 1, 1000, 1)
    per_page  = st.slider("í•œ ë²ˆì— ê°€ì ¸ì˜¬ ê±´ìˆ˜", 1, 1000, 100)
    if st.button("ğŸ” ì¡°íšŒ"):
        params = {
            "serviceKey": API_KEY,
            "page":       page,
            "perPage":    per_page,
            "fromDate":   date_from.strftime("%Y-%m-%d"),
            "toDate":     date_to.strftime("%Y-%m-%d"),
        }
        with st.spinner("ì¡°íšŒ ì¤‘â€¦"):
            res = requests.get(
                "https://api.odcloud.kr/api/15128156/v1/uddi:fdcdb0d1-0296-4c3b-8087-8ab4bd4d5123",
                params=params
            )
        if res.status_code != 200:
            st.error(f"API ì˜¤ë¥˜ {res.status_code}")
            return
        data = res.json().get("data", [])
        if data:
            df = pd.DataFrame(data)
            st.success(f"ì´ {len(df)} ê±´ ì¡°íšŒë˜ì—ˆìŠµë‹ˆë‹¤.")
            st.dataframe(df)
        else:
            st.warning("ì¡°íšŒëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

# â”€â”€â”€ 3) ì˜¤ëŠ˜ì˜ ë‚ ì”¨ ì„¹ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def today_weather_section():
    st.subheader("â˜€ï¸ ì˜¤ëŠ˜ì˜ ë‚ ì”¨ ì¡°íšŒ")
    city_name = st.text_input("ë„ì‹œ ì´ë¦„ ì…ë ¥ (ì˜ˆ: ì„œìš¸, Busan)")
    if st.button("ğŸ” ë‚ ì”¨ ê°€ì ¸ì˜¤ê¸°"):
        if not city_name:
            st.warning("ë„ì‹œ ì´ë¦„ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
            return

        q_name  = quote_plus(city_name)
        geo_url = f"https://geocoding-api.open-meteo.com/v1/search?name={q_name}&count=5&language=ko"
        with st.spinner("ìœ„ì¹˜ ê²€ìƒ‰ ì¤‘â€¦"):
            geo_res = requests.get(geo_url)
        if geo_res.status_code != 200:
            st.error("ì§€ì˜¤ì½”ë”© API ì˜¤ë¥˜")
            return
        results = geo_res.json().get("results")
        if not results:
            st.warning("ë„ì‹œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        loc          = results[0]
        lat, lon     = loc["latitude"], loc["longitude"]
        display_name = f"{loc['name']}, {loc['country']}"

        weather_url = (
            f"https://api.open-meteo.com/v1/forecast?"
            f"latitude={lat}&longitude={lon}&current_weather=true"
            f"&hourly=relativehumidity_2m&timezone=auto"
        )
        with st.spinner(f"{display_name} ë‚ ì”¨ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘â€¦"):
            w_res = requests.get(weather_url)
        if w_res.status_code != 200:
            st.error("ë‚ ì”¨ API ì˜¤ë¥˜")
            return

        js   = w_res.json()
        cw   = js.get("current_weather", {})
        temp, wind_spd, wind_dir, code = (
            cw.get("temperature"),
            cw.get("windspeed"),
            cw.get("winddirection"),
            cw.get("weathercode"),
        )
        wc_map = {
            0:"ë§‘ìŒ",1:"ì£¼ë¡œ ë§‘ìŒ",2:"ë¶€ë¶„ì  êµ¬ë¦„",3:"êµ¬ë¦„ ë§ìŒ",
            45:"ì•ˆê°œ",48:"ì•ˆê°œ(ì…ìƒ)",
            51:"ì´ìŠ¬ë¹„ ì•½í•¨",53:"ì´ìŠ¬ë¹„ ë³´í†µ",55:"ì´ìŠ¬ë¹„ ê°•í•¨",
            61:"ë¹—ë°©ìš¸ ì•½í•¨",63:"ë¹—ë°©ìš¸ ë³´í†µ",65:"ë¹—ë°©ìš¸ ê°•í•¨",
            80:"ì†Œë‚˜ê¸° ì•½í•¨",81:"ì†Œë‚˜ê¸° ë³´í†µ",82:"ì†Œë‚˜ê¸° ê°•í•¨",
            95:"ë‡Œìš°",96:"ì•½í•œ ë‡Œìš°",99:"ê°•í•œ ë‡Œìš°"
        }
        desc      = wc_map.get(code, "ì•Œ ìˆ˜ ì—†ìŒ")
        times     = js["hourly"]["time"]
        hums      = js["hourly"]["relativehumidity_2m"]
        now       = datetime.now().strftime("%Y-%m-%dT%H:00")
        humidity  = hums[times.index(now)] if now in times else None

        st.markdown(f"### {display_name} í˜„ì¬ ë‚ ì”¨")
        c1, c2, c3, c4 = st.columns(4)
        c1.metric("ğŸŒ¡ï¸ ê¸°ì˜¨(â„ƒ)", temp)
        c2.metric("ğŸ’¨ í’ì†(m/s)", wind_spd)
        c3.metric("ğŸŒ«ï¸ í’í–¥(Â°)", wind_dir)
        c4.metric("ğŸ’§ ìŠµë„(%)", humidity or "â€“")
        st.markdown(f"**ë‚ ì”¨ ìƒíƒœ:** {desc}")

# â”€â”€â”€ 4) Chatbot (Vision) & ìš”ì•½ ê¸°ëŠ¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
enc = tiktoken.encoding_for_model("gpt-4o-mini")
MAX_TOKENS = 131072  # gpt-4o-mini ìµœëŒ€ í† í° í—ˆìš©ì¹˜ (ì•½ 131K)
SUMMARY_THRESHOLD = 40  # ëŒ€í™” ë©”ì‹œì§€(turn)ê°€ 40ê°œ ì´ìƒ ë„˜ì–´ê°€ë©´ ìš”ì•½

def num_tokens(messages: list) -> int:
    total = 0
    for m in messages:
        if isinstance(m["content"], list):
            for blk in m["content"]:
                if blk["type"] == "text":
                    total += len(enc.encode(blk["text"]))
                elif blk["type"] == "image_url":
                    total += len(enc.encode(blk["image_url"]["url"]))
        else:
            total += len(enc.encode(m["content"]))
    return total

@backoff.on_exception(backoff.expo, openai.RateLimitError, max_time=60, jitter=None)
def safe_chat_completion(messages, model="gpt-4o-mini"):
    tk_in = num_tokens(messages)
    if tk_in > MAX_TOKENS:
        raise ValueError(f"ì…ë ¥ í† í° {tk_in}ê°œ â†’ ìµœëŒ€ í—ˆìš©ì¹˜({MAX_TOKENS}) ì´ˆê³¼ì…ë‹ˆë‹¤.")
    return openai.chat.completions.create(
        model=model,
        messages=messages,
        max_tokens=300,
        stream=True
    )

def compress_image(file, max_px=768, quality=85):
    img = Image.open(file)
    if max(img.size) > max_px:
        ratio = max_px / max(img.size)
        img = img.resize((int(img.width * ratio), int(img.height * ratio)), Image.LANCZOS)
    buf = BytesIO()
    img.convert("RGB").save(buf, format="JPEG", quality=quality)
    return buf.getvalue()

def summarize_history(history: list) -> str:
    """
    ì „ì²´ history(messages)ë¥¼ ì§§ê²Œ ìš”ì•½í•˜ì—¬ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜.
    GPTì—ê²Œ ìš”ì•½ ìš”ì²­ì„ ë³´ë‚´ê³  3ë¬¸ì¥ ì´ë‚´ ìš”ì•½ë¬¸ì„ ë°›ì•„ì˜´.
    """
    prompt = [{"role": "system", "content": "ì•„ë˜ ëŒ€í™”ë¥¼ ì§§ê²Œ ìš”ì•½í•´ ì£¼ì„¸ìš”."}]
    prompt += history + [{"role": "user", "content": "ì, ì´ ëŒ€í™” ë‚´ìš©ì„ 3ë¬¸ì¥ ì´ë‚´ë¡œ ìš”ì•½í•´ ì¤˜."}]
    res = openai.chat.completions.create(
        model="gpt-4o-mini",
        messages=prompt,
        max_tokens=200
    )
    return res.choices[0].message.content  # ìš”ì•½ëœ í…ìŠ¤íŠ¸

def chatgpt_clone_section():
    st.subheader("ğŸ’¬ Chatbot (Vision)")
    img_file = st.file_uploader("ğŸ–¼ï¸ ì´ë¯¸ì§€ (ì„ íƒ)", type=["png", "jpg", "jpeg"])
    prompt   = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”")

    # ì„¸ì…˜ ìŠ¤í…Œì´íŠ¸ ì´ˆê¸°í™”
    st.session_state.setdefault("chat_history", [])

    # 1) ëŒ€í™” ê¸°ë¡ì´ SUMMARY_THRESHOLD í„´ì„ ì´ˆê³¼í•˜ë©´ ìš”ì•½ ìˆ˜í–‰
    if len(st.session_state.chat_history) > SUMMARY_THRESHOLD:
        try:
            summary = summarize_history(st.session_state.chat_history)
            # ìš”ì•½ëœ í…ìŠ¤íŠ¸ë¥¼ assistant ì—­í• ë¡œ ì €ì¥ í›„, historyë¥¼ ì¬êµ¬ì„±
            st.session_state.chat_history = [
                {"role": "assistant", "content": summary}
            ]
        except Exception as e:
            st.error(f"ëŒ€í™” ìš”ì•½ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            return

    # 2) í™”ë©´ì— ì´ì „ ëŒ€í™” ë‚´ìš© í‘œì‹œ
    for msg in st.session_state.chat_history:
        role = msg["role"]
        content = msg["content"]
        if role == "user":
            with st.chat_message("user"):
                # contentê°€ ë¸”ë¡ ë¦¬ìŠ¤íŠ¸ì¼ ê²½ìš° ì²˜ë¦¬
                if isinstance(content, list):
                    for blk in content:
                        if blk["type"] == "text":
                            st.write(blk["text"])
                        elif blk["type"] == "image_url":
                            st.image(blk["image_url"]["url"], caption="ì‚¬ìš©ì ì—…ë¡œë“œ ì´ë¯¸ì§€")
                else:
                    st.write(content)
        else:  # assistant
            with st.chat_message("assistant"):
                st.write(content)

    # 3) ì…ë ¥ì´ ì—†ìœ¼ë©´ ë¦¬í„´
    if img_file is None and not prompt:
        return

    # 4) ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
    user_blocks = []
    if prompt:
        user_blocks.append({"type": "text", "text": prompt})
    if img_file:
        # compress_image í•¨ìˆ˜ëŠ” max_px=768, quality=85 ê³ ì •
        jpg_bytes = compress_image(img_file)
        st.image(jpg_bytes, caption=f"ë¯¸ë¦¬ë³´ê¸° ({len(jpg_bytes)//1024} KB)", use_container_width=True)
        b64 = base64.b64encode(jpg_bytes).decode()
        img_block = {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{b64}"}}
        user_blocks.append(img_block)

    # 5) ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ chat_historyì— ì¶”ê°€
    st.session_state.chat_history.append({"role": "user", "content": user_blocks})

    # 6) ëª¨ë¸ì—ê²Œ ë³´ë‚¼ ë©”ì‹œì§€(prospective)ë¥¼ êµ¬ì„± í›„ í† í° ê²€ì¦
    prospective = st.session_state.chat_history.copy()
    tk_in = num_tokens(prospective)
    if tk_in > MAX_TOKENS:
        st.error(f"í˜„ì¬ ëŒ€í™” í† í° ìˆ˜({tk_in})ê°€ ë„ˆë¬´ ë§ì•„ í˜¸ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
                 "ì˜¤ë˜ëœ ëŒ€í™”ë¥¼ ìš”ì•½í•˜ê±°ë‚˜ ì¼ë¶€ ë©”ì‹œì§€ë¥¼ ì œê±°í•´ ì£¼ì„¸ìš”.")
        return

    # 7) GPT í˜¸ì¶œ ë° ì¶œë ¥
    try:
        resp = safe_chat_completion(st.session_state.chat_history)
        buf = ""
        # assistant ë©”ì‹œì§€ë¥¼ ë¯¸ë¦¬ ì¶”ê°€í•´ë‘ê³ , ë‚´ìš©ì„ ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ê³„ì† ë§ë¶™ì„
        st.session_state.chat_history.append({"role": "assistant", "content": ""})
        with st.chat_message("assistant"):
            ph = st.empty()
            for chunk in resp:
                delta = chunk.choices[0].delta.content
                if delta:
                    buf += delta
                    ph.markdown(buf + "â–Œ")
            ph.markdown(buf)
        # ì™„ì„±ëœ assistant ì‘ë‹µì„ ì„¸ì…˜ì— ë°˜ì˜
        st.session_state.chat_history[-1]["content"] = buf
    except openai.RateLimitError:
        st.error("â³ ë ˆì´íŠ¸ ë¦¬ë°‹ì— ê±¸ë ¸ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")
    except Exception as e:
        st.error(f"OpenAI í˜¸ì¶œ ì˜¤ë¥˜: {e}")

# â”€â”€â”€ 5) ì˜ìƒ ëª¨ìŒ ì„¹ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def video_collection_section():
    st.subheader("ğŸ“º ESG ì˜ìƒ ëª¨ìŒ")
    # 1. ì‚¬ë¬´ì‹¤ì—ì„œ ì´ë©´ì§€ í™œìš©í•˜ê¸°!
    st.markdown("#### ì‚¬ë¬´ì‹¤ì—ì„œ ì´ë©´ì§€ í™œìš©í•˜ê¸°!")
    st.video("https://storage.googleapis.com/videoupload_icpa/%EC%82%AC%EB%AC%B4%EC%8B%A4%EC%97%90%EC%84%9C%20%EC%9D%B4%EB%A9%B4%EC%A7%80%20%ED%99%9C%EC%9A%A9%ED%95%98%EA%B8%B0.mp4")
    st.write("")  # ì¤„ ê°„ê²©

    # 2. ì¹´í˜ì—ì„œ ESG ì‹¤ì²œí•˜ê¸° 1íƒ„
    st.markdown("#### ì¹´í˜ì—ì„œ ESG ì‹¤ì²œí•˜ê¸° 1íƒ„")
    st.video("https://storage.googleapis.com/videoupload_icpa/%EC%B9%B4%ED%8E%98%EC%97%90%EC%84%9C%20%ED%85%80%EB%B8%94%EB%9F%AC%20%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0.mp4")
    st.write("")

    # 3. ì¹´í˜ì—ì„œ íœ´ì§€ ì ê²Œ ì‚¬ìš©í•˜ê¸°
    st.markdown("#### ì¹´í˜ì—ì„œ íœ´ì§€ ì ê²Œ ì‚¬ìš©í•˜ê¸°")
    st.video("https://storage.googleapis.com/videoupload_icpa/%EC%B9%B4%ED%8E%98%EC%97%90%EC%84%9C%20%ED%9C%B4%EC%A7%80%20%EC%A0%81%EA%B2%8C%20%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0.mp4")

# â”€â”€â”€ 6) ì•± ë ˆì´ì•„ì›ƒ (íƒ­ êµ¬ì„±) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="í†µí•© ë°ëª¨", layout="centered")
st.title("ğŸ“ˆ í†µí•© ë°ëª¨: ë‰´ìŠ¤Â·ì„ ë°•Â·ë‚ ì”¨Â·ChatbotÂ·ì˜ìƒ ëª¨ìŒ")

tabs = st.tabs([
    "êµ¬ê¸€ ë‰´ìŠ¤", "ì„ ë°• ê´€ì œì •ë³´", "ì˜¤ëŠ˜ì˜ ë‚ ì”¨", "Chatbot", "ESG ì˜ìƒ ëª¨ìŒ"
])

with tabs[0]:
    st.subheader("â–¶ êµ¬ê¸€ ë‰´ìŠ¤ í¬ë¡¤ë§ (RSS)")
    kw  = st.text_input("ê²€ìƒ‰ í‚¤ì›Œë“œ", "ESG")
    num = st.slider("ê°€ì ¸ì˜¬ ê¸°ì‚¬ ê°œìˆ˜", 5, 50, 10)
    if st.button("ë³´ê¸°", key="news_btn"):
        for it in fetch_google_news(kw, num):
            st.markdown(f"- **[{it['source']} Â· {it['date']}]** [{it['title']}]({it['link']})")

with tabs[1]:
    vessel_monitoring_section()

with tabs[2]:
    today_weather_section()

with tabs[3]:
    chatgpt_clone_section()

with tabs[4]:
    video_collection_section()













